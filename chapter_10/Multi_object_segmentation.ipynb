{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PacktPublishing/Modern-Computer-Vision-with-PyTorch-2E/blob/main/Chapter10/Multi_object_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O6zkXVmBwyRi"
   },
   "outputs": [],
   "source": [
    "# install the required packages\n",
    "%pip install -qU openimages torch_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5dWAJWpew-Rp"
   },
   "outputs": [],
   "source": [
    "#download the required annotation files\n",
    "from torch_snippets import *\n",
    "!wget -O train-annotations-object-segmentation.csv -q https://storage.googleapis.com/openimages/v5/train-annotations-object-segmentation.csv\n",
    "!wget -O classes.csv -q https://raw.githubusercontent.com/openimages/dataset/master/dict.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "t3ZsaZyd9EkT"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch_snippets import *\n",
    "from builtins import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PBI7wUaow_0w"
   },
   "outputs": [],
   "source": [
    "#specify the classes that we want the model to predict\n",
    "required_classes = 'person,dog,bird,car,elephant,football,jug,laptop,mushroom,pizza,rocket,shirt,traffic sign,watermelon,zebra'\n",
    "required_classes = [c.lower() for c in required_classes.lower().split(',')]\n",
    "\n",
    "classes = pd.read_csv('classes.csv', header=None)\n",
    "classes.columns = ['class','class_name']\n",
    "classes = classes[classes['class_name'].map(lambda x: x in required_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o_jIb7eGxDU5"
   },
   "outputs": [],
   "source": [
    "#fetch the image IDs and masks corresponding to required_classes\n",
    "from torch_snippets import *\n",
    "df = pd.read_csv('train-annotations-object-segmentation.csv')\n",
    "df.head()\n",
    "\n",
    "data = pd.merge(df, classes, left_on='LabelName', right_on='class')\n",
    "\n",
    "#only fetching 500 images per class in buset_data\n",
    "subset_data = data.groupby('class_name').agg({'ImageID': lambda x: list(x)[:500]})\n",
    "subset_data = flatten(subset_data.ImageID.tolist())\n",
    "subset_data = data[data['ImageID'].map(lambda x: x in subset_data)]\n",
    "subset_masks = subset_data['MaskPath'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dkhb9-IPxTW1",
    "outputId": "490c70e5-64a3-4623-d230-78883f0ca154"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 12%|█▎        | 2/16 [03:43<25:27, 109.08s/it]"
     ]
    }
   ],
   "source": [
    "#now that we have the subset of masks data to download, lets start the download.,\n",
    "# we have to run this step once for each of the 16 files\n",
    "!mkdir -p masks\n",
    "for c in Tqdm('0123456789abcdef'):\n",
    "    !wget -q https://storage.googleapis.com/openimages/v5/train-masks/train-masks-{c}.zip\n",
    "    !unzip -q train-masks-{c}.zip -d tmp_masks\n",
    "    !rm train-masks-{c}.zip\n",
    "    tmp_masks = Glob('tmp_masks', silent=True)\n",
    "    items = [(m,fname(m)) for m in tmp_masks]\n",
    "    items = [(i,j) for (i,j) in items if j in subset_masks]\n",
    "    for i,j in items:\n",
    "        os.rename(i, f'masks/{j}')\n",
    "    !rm -rf tmp_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GH6WAiisxhIW"
   },
   "outputs": [],
   "source": [
    "#download the images corresponding to ImageId\n",
    "masks = Glob('masks')\n",
    "masks = [fname(mask) for mask in masks]\n",
    "\n",
    "subset_data = subset_data[subset_data['MaskPath'].map(lambda x: x in masks)]\n",
    "subset_imageIds = subset_data['ImageID'].tolist()\n",
    "\n",
    "from openimages.download import _download_images_by_id\n",
    "!mkdir images\n",
    "_download_images_by_id(subset_imageIds, 'train', './images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7N15Isujxscb"
   },
   "outputs": [],
   "source": [
    "# zip all images, masks, and ground truths and save them - just in case our session crashes\n",
    "import zipfile\n",
    "files = Glob('images') + Glob('masks') + ['train-annotations-object-segmentation.csv', 'classes.csv']\n",
    "with zipfile.ZipFile('data.zip','w') as zipme:\n",
    "    for file in Tqdm(files):\n",
    "        zipme.write(file, compress_type=zipfile.ZIP_DEFLATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03rupy02xssq"
   },
   "outputs": [],
   "source": [
    "# move data into a singe directory\n",
    "!mkdir -p train/\n",
    "!mv images train/myData2020\n",
    "!mv masks train/abnnotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBpgR64gxuQ1"
   },
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "!git clone https://github.com/sizhky/pycococreator.git\n",
    "%cd pycococreator\n",
    "%pip install -e .\n",
    "%cd -\n",
    "%pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xC-ID-NRUg-M"
   },
   "outputs": [],
   "source": [
    "# define the required categories in COCO format\n",
    "import datetime\n",
    "INFO = {\n",
    "    \"description\": \"MyData2020\",\n",
    "    \"url\": \"None\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"year\": 2020,\n",
    "    \"contributor\": \"sizhky\",\n",
    "    \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
    "}\n",
    "\n",
    "LICENSES = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"MIT\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# we are not interested in supercategories and so we will specify it as none\n",
    "CATEGORIES = [{'id': id+1, 'name': name.replace('/',''), 'supercategory': 'none'} for id,(_,(name, clss_name)) in enumerate(classes.iterrows())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OLxzKH7x2SR"
   },
   "outputs": [],
   "source": [
    "# import the relevant packages\n",
    "from pycococreator.pycococreatortools import pycococreatortools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "\n",
    "# create an empty dictionary with the keys needed to save the COCO JSON file\n",
    "coco_output = {\n",
    "    \"info\": INFO,\n",
    "    \"licenses\": LICENSES,\n",
    "    \"categories\": CATEGORIES,\n",
    "    \"images\": [],\n",
    "    \"annotations\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJfdU5tjx8vd"
   },
   "outputs": [],
   "source": [
    "#set a few vairables in place that contain the info on the image locations and annotation file locations\n",
    "ROOT_DIR = \"train\"\n",
    "IMAGE_DIR, ANNOTATION_DIR = 'train/myData2020/', 'train/annotations/'\n",
    "image_files = [f for f in listdir(IMAGE_DIR) if isfile(join(IMAGE_DIR, f))]\n",
    "annotation_files = [f for f in listdir(ANNOTATION_DIR) if isfile(join(ANNOTATION_DIR, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AC5BdfNx_Zj"
   },
   "outputs": [],
   "source": [
    "# loop through each image filename and populate the images key in the coco_output dictionary\n",
    "image_id = 1\n",
    "# go through each image\n",
    "for image_filename in Tqdm(image_files):\n",
    "    image = Image.open(IMAGE_DIR + '/' + image_filename)\n",
    "    image_info = pycococreatortools.create_image_info(image_id, os.path.basename(image_filename), image.size)\n",
    "    coco_output[\"images\"].append(image_info)\n",
    "    image_id = image_id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaK7kCT9yLy2"
   },
   "outputs": [],
   "source": [
    "# loop through each segmentation annotations and populate the annotations key in the coco_output dictionary\n",
    "segmentation_id = 1\n",
    "for annotation_filename in Tqdm(annotation_files):\n",
    "    image_id = [f for f in coco_output['images'] if stem(f['file_name'])==annotation_filename.split('_')[0]][0]['id']\n",
    "    class_id = [x['id'] for x in CATEGORIES if x['name'] in annotation_filename][0]\n",
    "    category_info = {'id': class_id, 'is_crowd': 'crowd' in image_filename}\n",
    "    binary_mask = np.asarray(Image.open(f'{ANNOTATION_DIR}/{annotation_filename}').convert('1')).astype(np.uint8)\n",
    "\n",
    "    annotation_info = pycococreatortools.create_annotation_info(segmentation_id, image_id, category_info, binary_mask, image.size, tolerance=2)\n",
    "\n",
    "    if annotation_info is not None:\n",
    "        coco_output[\"annotations\"].append(annotation_info)\n",
    "        segmentation_id = segmentation_id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QshA9U_lyPlv"
   },
   "outputs": [],
   "source": [
    "#save coco_output in a json file\n",
    "coco_output['categories'] = [{'id': id+1, 'name': clss_name, 'supercategory': 'none'} for id,(_,(name, clss_name)) in enumerate(classes.iterrows())]\n",
    "\n",
    "import json\n",
    "with open('images.json', 'w') as output_json_file:\n",
    "    json.dump(coco_output, output_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgKDlPCeyRCP"
   },
   "outputs": [],
   "source": [
    "# install the required detectron2 packages\n",
    "%cd /content/\n",
    "# install detectron2:\n",
    "!git clone https://github.com/facebookresearch/detectron2\n",
    "%cd /content/detectron2\n",
    "%pip install -r requirements.txt\n",
    "!python setup.py install\n",
    "%cd /content\n",
    "!git clone https://github.com/facebookresearch/fvcore.git\n",
    "%cd /content/fvcore\n",
    "%pip install -e .\n",
    "%cd /content/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvBt1S_xLSaq"
   },
   "outputs": [],
   "source": [
    "# given that we have restarted colab, lets refetch the required classes\n",
    "from torch_snippets import *\n",
    "required_classes = 'person,dog,bird,car,elephant,football,jug,laptop,mushroom,pizza,rocket,shirt,traffic sign,watermelon,zebra'\n",
    "required_classes = [c.lower() for c in required_classes.lower().split(',')]\n",
    "\n",
    "classes = pd.read_csv('classes.csv', header=None)\n",
    "classes.columns = ['class','class_name']\n",
    "classes = classes[classes['class_name'].map(lambda x: x in required_classes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42_JaPvtUg-R"
   },
   "source": [
    "restart the notebook if below cell fails to execute,  \n",
    "and start running from above cell  \n",
    "no need to run from the very top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgowdCn3ytx5"
   },
   "outputs": [],
   "source": [
    "# import the relevant detectron2 packages\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.engine import DefaultTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72_l7xUW81pR"
   },
   "outputs": [],
   "source": [
    "#register the created datasets\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "register_coco_instances(\"dataset_train\", {}, \"images.json\", \"train/myData2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ri6QC_Oi84WV"
   },
   "outputs": [],
   "source": [
    "#define all the parameters in the cfg configuration file, cfg is a special detectron object that holds all the relevant info for training a model\n",
    "cfg = get_cfg()\n",
    "# import all the core parameters from a pre-existing configuration file that was used for pretraining mask_rcnn with FPN as the backbone\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"dataset_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\") # pretrained weights\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025 # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 5000 # instead of epochs, we train on 5000 batches\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYe7DxIH86Fh"
   },
   "outputs": [],
   "source": [
    "#train the model to predict classes, bounding boxes, and also the segmentation of objecgs belonging to the defined classes within out custom dataset\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8Rj9SObGi8z"
   },
   "outputs": [],
   "source": [
    "### making inferences on a new image - we load the path, set the probablity threshold and pass it thru the DefaultPredictot method\n",
    "\n",
    "# load the weights with the trained model - use the same cfg and load the model weights\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "# set the threshold for the probability of the object belonging to a certain class\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.25\n",
    "# define the predictor method\n",
    "predictor = DefaultPredictor(cfg)\n",
    "# perform segmentation on the image of interest and visualize it - we are randomly plotting 30 training images\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "files = Glob('train/myData2020')\n",
    "for _ in range(30):\n",
    "    im = cv2.imread(str(choose(files)))\n",
    "    outputs = predictor(im)\n",
    "    # visualizer is detectron2's way of plotting onject instances\n",
    "    v = Visualizer(im[:, :, ::-1], # the image we want to visualize\n",
    "                    scale=0.5, # the size of the image when plotter- we are shrinking the image down to 50%\n",
    "                    metadata=MetadataCatalog.get(\"dataset_train\"), # class leve info for the dataset - index-to-class mapping so that when we send the raw tensors as input to be plotted, the class will decode them into actual human-readable classes\n",
    "                    instance_mode=ColorMode.IMAGE_BW # we are asking the model to only hightlight the segmented pixels\n",
    "    # remove the colors of unsegmented pixels.\n",
    "    # This option is only available for segmentation models\n",
    "    )\n",
    "\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    show(out.get_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJNdsxuVLI9D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
